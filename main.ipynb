{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0449249f",
   "metadata": {},
   "source": [
    "1. Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeef3769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.cuda\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "ROOT = Path(\"./data\")\n",
    "if not ROOT.exists():\n",
    "    ROOT.mkdir()\n",
    "    \n",
    "CUDA_AVAILABLE = torch.cuda.is_available()\n",
    "\n",
    "DATASET_PATH = ROOT / \"vggface2\"\n",
    "if not DATASET_PATH.exists():\n",
    "    DATASET_PATH.mkdir()\n",
    "    with requests.get(\"https://www.kaggle.com/api/v1/datasets/download/hearfool/vggface2\", stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        tmp = DATASET_PATH / \"tmp\"\n",
    "        with tmp.open(\"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "    with zipfile.ZipFile(str(tmp)) as zip_ref:\n",
    "        zip_ref.extractall(str(DATASET_PATH))\n",
    "    tmp.unlink()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8ce8ca",
   "metadata": {},
   "source": [
    "2. Compute embbeding vector for each image on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37671acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: data/insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: data/insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: data/insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: data/insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: data/insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "TOOK: 0.609874851998029\n"
     ]
    }
   ],
   "source": [
    "from time import monotonic\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from IPython.display import display\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from insightface.app import FaceAnalysis\n",
    "from torch import nn\n",
    "\n",
    "app = FaceAnalysis(root=str(ROOT / \"insightface\"), name='buffalo_l', providers=['CPUExecutionProvider' if not CUDA_AVAILABLE else 'CUDAExecutionProvider'])  # Use 'CUDAExecutionProvider' for GPU\n",
    "app.prepare(ctx_id=(-1 if not CUDA_AVAILABLE else 0))  # ctx_id=-1 for CPU, 0 for GPU\n",
    "\n",
    "pil_transform = transforms.ToPILImage()\n",
    "t = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "])\n",
    "\n",
    "def from_tensor_to_cv2_like(tensor):\n",
    "    return cv2.cvtColor(np.array(tensor), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def show_images(images, titles):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, (img, title) in enumerate(zip(images, titles)):\n",
    "        plt.subplot(2, 2, i + 1)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img_rgb)\n",
    "        plt.title(title)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "vggface2 = ImageFolder(DATASET_PATH / \"val\", transform=t)\n",
    "subset = Subset(vggface2, range(100))\n",
    "loader = DataLoader(subset, batch_size=10, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "begin = monotonic()\n",
    "embedding2 = app.get(cv2.imread(\"./data/vggface2/val/n000001/0001_01.jpg\"))[0].embedding\n",
    "print(f\"TOOK: {embedding2}\")\n",
    "\n",
    "\n",
    "#def compare_faces(emb1, emb2, threshold=0.65): # Adjust this threshold according to your usecase.\n",
    "#    \"\"\"Compare two embeddings using cosine similarity\"\"\"\n",
    "#    similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "#    return similarity, similarity > threshold\n",
    "#\n",
    "#print(compare_faces(embedding2, embedding3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bde9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
